<!DOCTYPE html>
<html lang="en">
<head prefix="og: http://ogp.me/ns#">
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1" />
  <meta property="og:title" content=" Transfer Learning - Sneak Peak &middot;  Data Science Wiki" />
  
    <meta name="theme-color" content="#hexcolor" />
  
  <meta property="og:site_name" content="Data Science Wiki" />
  <meta property="og:url" content="/posts/transfer-learning/" />
  
  
    <meta property="og:type" content="article" />
    
    <meta property="og:article:published_time" content="2020-04-13T22:00:00Z" />
    
      <meta property="og:article:tag" content="Machine Learning" />
    
  

  <title>
     Transfer Learning - Sneak Peak &middot;  Data Science Wiki
  </title>

  <link rel="alternative stylesheet" href="/css/bootstrap.min.css" />
  
  <link rel="stylesheet" href="/css/all.css">
  <link rel="stylesheet" href="/css/main.css" />
  <link rel="stylesheet" href="/css/github.css" />
  <link rel="stylesheet" href="/css/color-theme.css" />
  <link rel="stylesheet" href="//fonts.googleapis.com/css?family=Source+Sans+Pro:200,300,400" type="text/css">
  <link rel="shortcut icon" href="/images/favicon.ico" />
  <link rel="apple-touch-icon" href="/images/apple-touch-icon.png" />

  

  
</head>
<body>
  
  <header class="global-header"  style="background-image:url(/images/bg-dark.png)">
  
    <section class="header-text">
      <h1><a href="/">Data Science Wiki</a></h1>
      
        <h4 class="tag-line">
          <i>&#34;All data has its beauty, but not everyone sees it &#34; - Damian Mingle </i>
        </h4>
      
      
        <a class="btn btn-default btn-home" href="/">
          <i class="fa fa-angle-left" aria-hidden="true"></i>
          &nbsp;Home
        </a>
      
      
      <div class="navbar-container">
        
          <a class="btn btn-default navbar-item" href="/pages/about">
            About
          </a>
        
          <a class="btn btn-default navbar-item" href="/pages/glossary">
            Glossary
          </a>
        
          <a class="btn btn-default navbar-item" href="/pages/links">
            Study Repository
          </a>
        
        
        <div class="sns-links hidden-print">
  
    <a href="mailto:gomes.inesisabel@gmail.com">
      <i class="fa fa-envelope"></i>
    </a>
  
  
  
  
  
  
    <a href="https://github.com/inesgomes" target="_blank">
      <i class="fab fa-github"></i>
    </a>
  
  
  
  
  
  
    <a href="https://linkedin.com/in/inesgomes778" target="_blank">
      <i class="fab fa-linkedin"></i>
    </a>
  
  
  <a href="https://www.kaggle.com/inesgomes" target="_blank">
    <i class="fab fa-kaggle"></i>
  </a>
  
  
</div>

      </div>
      
      
      
      
      
      
      
      
      
      
      
    </section>
  </header>
  <main class="container">



<article>
  <header class="article-title">
    <h2 class="text-primary">Transfer Learning - Sneak Peak</h2>
  </header>
  <div class="delimiter"></div>
   
<aside>
    <h3>Table of Contents:</h3>
    <nav id="TableOfContents">
<ul>
<li><a href="#transfer-learning-methods">Transfer Learning Methods</a>
<ul>
<li><a href="#deep-learning">Deep Learning</a></li>
</ul></li>
<li><a href="#advantages-and-limitations">Advantages and Limitations</a></li>
<li><a href="#final-remarks">Final Remarks</a>
<ul>
<li><a href="#related-research-areas">Related Research Areas</a></li>
<li><a href="#extra-resources">Extra Resources</a></li>
</ul></li>
<li><a href="#bibliography">Bibliography</a></li>
</ul>
</nav>
    <div class="delimiter"></div>
</aside>

  <section>
    

<p>Transfer Learning (TL) is a field of Machine Learning (ML) that focus on gaining knowledge from data and transfer it to new domains. The term, coined in 1995 during the NIPS conference, was popularized in 2016 by Andrew Ng when he stated, during a NIPS conference tutorial, that Transfer Learning is &ldquo;<em>the next driver of ML commercial success</em>&rdquo;.</p>

<p>Actually, given the infinite number of new scenarios available in the real world, transfer learning is very valuable, as it leverages the already existing data and generalizes with ease to novel situations. As an example, transfer learning is capable of identifying pedestrians on nightlight images, using training data daylight pedestrian images! If we’d try to fit a traditional ML model using this set of training/test images, the model would suffer from inherit data bias, not being able to generalize and consequently lowering the performance.</p>

<p>The following illustration from [<a href="#bibliography">1</a>] shows the difference between traditional ML and Transfer Learning. While the former implies that each domain must be trained by a different model, transfer learning techniques allow to extract the knowledge from previous domains to apply on the new one.</p>

<p><center>
<img src="../images/transfer-learning.png" alt="drawing" width="550"/></p>

<p><em>Figure 1: Traditional Machine Learning (a) and Transfer Learning (b) illustration by Pan and Yang [<a href="#bibliography">2</a>].</em>
</center></p>

<h1 id="transfer-learning-methods">Transfer Learning Methods</h1>

<p>In transfer learning, there are three main research issues:</p>

<ul>
<li><strong>What to transfer?</strong> asks which part of knowledge can be transferred across domains or task. We may transfer: <em>instance</em>, <em>feature-representation</em>, <em>parameter</em> or <em>relational-knowledge</em>;</li>
<li><strong>When to transfer?</strong> asks in which situations transferring skills should be done. It&rsquo;s particularly important to understand when <strong>not</strong> to transfer, that is, when using transfer learning may hurt the performance (i.e., negative transfer);</li>
<li><strong>How to transfer?</strong> asks which learning algorithm needs to be developed.</li>
</ul>

<p>Then, given the previous questions we try to fit our problem in one of the transfer learning scenarios [<a href="#bibliography">2</a>]:</p>

<ul>
<li><em>Transductive transfer learning</em>: the source and target tasks are the same, while the source and target domains are different.

<ul>
<li>the feature spaces are different, e.g. cross-lingual adaptation;</li>
<li>the marginal probability distributions are different, i.e. domain adaptation;</li>
</ul></li>
<li><em>Inductive transfer learning</em>: the target task is different from the source task, no matter when the source and target domains are the same or not;

<ul>
<li>the label spaces between the two tasks are different;</li>
</ul></li>
<li><em>Unsupervised Learning</em>: the target task is different from but related to the source task and there are no labelled data available in both source and target domains.</li>
</ul>

<p>Figure 2 sums up the previous scenarios and Table 1 summarizes the relationship between <em>what</em> and <em>how</em> to transfer.</p>

<p><center>
<img src="../images/transfer-learning-scenarios.png" alt="drawing" width="800"/></p>

<p><em>Figure 2: An overview of different transfer learning scenarios by Pan and Yang [<a href="#bibliography">1</a>].</em></p>

<p><br></p>

<table class="table">
  <thead>
  <tr>
    <th scope="col"></th>
    <th scope="col">Inductive TL<br></th>
    <th scope="col">Transductive TL<br></th>
    <th scope="col">Unsupervised TL</th>
  </tr>
  </thead>
  <tbody>
  <tr>
    <td scope="row">Instance</td>
    <td>✔</td>
    <td>✔</td>
    <td></td>
  </tr>
  <tr>
    <td scope="row">Feature Representation</td>
    <td>✔</td>
    <td>✔</td>
    <td>✔</td>
  </tr>
  <tr>
    <td scope="row">Parameter</td>
    <td>✔</td>
    <td></td>
    <td></td>
  </tr>
  <tr>
    <td scope="row">Relational knowledge<br></td>
    <td>✔</td>
    <td></td>
    <td></td>
  </tr>
  </tbody>
</table>

<p><em>Table 1: Different approaches used in different settings by Pan and Yang [<a href="#bibliography">1</a>].</em>
</center></p>

<h2 id="deep-learning">Deep Learning</h2>

<p>Deep learning (DL) models are representative of what is also known as inductive learning. Therefore, they can be used when the conditions for inductive learning apply. In this case, the goal is to use a pre-trained network and adapt it to make predictions on a new domain, instead of training a deep network from scratch.</p>

<p>An example of Deep Transfer Learning is explained in [<a href="#bibliography">3</a>]. In this case, a computer vision example is used to classify an image as rural or urban. The Kaggle instructor starts by explaining that, when using Deep Neural Networks for computer vision, the early layers of the model identify simple shapes, while the following ones identify more complex visual patterns and the very last make the predictions. For this reason, we can reuse the DL model by replacing the final layer by a new one that will be trained to classify an image as rural or urban. This is only possible because the previous layers provide useful information like the identification of fields, roads, houses, etc. The following illustration summarizes the described process.s.</p>

<p><center>
<img src="../images/dl-tl.gif" alt="gif" /></p>

<p><em>Figure 3: How to use a pre-trained Deep Learning Model to create new predictions with different but related domains (Inductive Transfer Learning) by Kaggle [<a href="#bibliography">3</a>]</em>
</center></p>

<p>The previous example is a special case of the <strong>Off-the-shelf Pre-trained Models as Feature Extractors</strong> technique. This method takes advantage of the layered structure of the pre-trained model&rsquo;s to leverage weighted layers and extract features. The weights of the model&rsquo;s layers are not updated during the training phase with new data for the new task. A more advanced technique is the <strong>Fine-Tuning Off-the-shelf Pre-trained Models</strong> that, not only replaces the final layer but also selectively retrain some of the previous ones. Further details on these methods may be found in Sarkar blogpost [<a href="#bibliography">4</a>].</p>

<h1 id="advantages-and-limitations">Advantages and Limitations</h1>

<p>After understanding the definition of Transfer Learning and its methods, in this section, we explore its main advantages and limitations.</p>

<p>The principal transfer learning advantage is enabling to build more robust models that can perform a wide variety of tasks [<a href="#bibliography">4</a>]. Likewise, Sarkar states the benefits of helping to solve complex real-world problems and tackling problems when having little labelled data availability.</p>

<p>On the counterpart, this technology still has some limitations, mainly related to the negative transfer (see <a href="#transfer-learning-methods">Transfer Learning Methods</a>), overfitting and transfer bounds (i.e., quantify the transfer) [<a href="#bibliography">4</a>] [<a href="#bibliography">5</a>].</p>

<h1 id="final-remarks">Final Remarks</h1>

<p>This is a brief introduction to transfer learning as the theme is very wide, with distinct strategies and applications that differ according to the problem in hands. As a matter of fact, this field is expanding at a large pace, so it&rsquo;s challenging to keep up-to-date 😊.</p>

<p>As a final note, Pan and Yang [<a href="#bibliography">1</a>] describe on their paper a <strong>formal transfer learning definition</strong> that involves the concepts of <em>domain</em> and <em>task</em>, that is not summarized in this post for the sake of simplicity. The paper is from 2010 so newer techniques and TL applied to DL are not covered. For TL using DL read [<a href="#bibliography">4</a>] blogpost.</p>

<h2 id="related-research-areas">Related Research Areas</h2>

<p>Sebastian Ruder [<a href="#bibliography">2</a>]  closes his blog post by stating that, there are several related or complementary research areas to the goals of transfer learning, introducing the following:</p>

<ul>
<li>semi-supervised learning;</li>
<li>multi-task learning;</li>
<li>continuous learning;</li>
<li>zero-shot learning.</li>
</ul>

<p>We&rsquo;ll keep an eye on them 😅!</p>

<h2 id="extra-resources">Extra Resources</h2>

<ul>
<li><a href="https://docs.google.com/presentation/d/1LsUAhR_qIVbq6xH6Aw4ag8MGB_-UWfd0KoVhtTgye6o/edit#slide=id.g6e76c30798_0_0">Transfer Learning in NLP - INRIA - ALMAnaCH</a>: State-of-the-art presentation for Natural Language Processing.</li>
<li>Link to <a href="https://www.cse.ust.hk/~qyang/Docs/2009/tkde_transfer_learning.pdf">A Survey on Transfer Learning</a> paper;</li>
<li><a href="https://dataskeptic.com/blog/episodes/2019/transfer_learning">Transfer Learning</a> podcast by Data Skeptic interviewing Sebastian Ruder;</li>
<li>Sarkar recommends <a href="https://github.com/dipanjanS/hands-on-transfer-learning-with-python">Hands-On Transfer Learning with Python</a> book.</li>
</ul>

<div class='delimiter'></div>

<h1 id="bibliography">Bibliography</h1>

<p><div class='bibliography'>
    [1] S. J. Pan and Q. Yang, &ldquo;A Survey on Transfer Learning,&rdquo; in IEEE Transactions on Knowledge and Data Engineering, vol. 22, no. 10, pp. 1345-1359, Oct. 2010.
    <br><br>
    [2] Sebastian Ruder. “Transfer Learning - Machine Learning&rsquo;s Next Frontier.” Sebastian Ruder. Sebastian Ruder, April 16, 2019. <a href="https://ruder.io/transfer-learning/">https://ruder.io/transfer-learning/</a>.
    <br><br>
    [3] Dansbecker. “Transfer Learning.” Kaggle. Kaggle, December 12, 2018. <a href="https://www.kaggle.com/dansbecker/transfer-learning">https://www.kaggle.com/dansbecker/transfer-learning</a>.
    <br><br>
    [4] Sarkar. “A Comprehensive Hands-on Guide to Transfer Learning with Real-World Applications in Deep Learning.” Medium. Towards Data Science, November 17, 2018. <a href="https://towardsdatascience.com/a-comprehensive-hands-on-guide-to-transfer-learning-with-real-world-applications-in-deep-learning-212bf3b2f27a">https://towardsdatascience.com/a-comprehensive-hands-on-guide-to-transfer-learning-with-real-world-applications-in-deep-learning-212bf3b2f27a</a>.
    <br><br>
    [5] Joshinav. “Exploring the Limits of Transfer Learning.” Application development, February 3, 2020. <a href="https://www.allerin.com/blog/exploring-the-limits-of-transfer-learning">https://www.allerin.com/blog/exploring-the-limits-of-transfer-learning</a>.
</div>
<br></p>

  </section>
  
  <div class="clearfix">
    
      <div class="post-date pull-left">
        <span class="small">
          Posted on
          Apr 13, 2020 at 22:00
        </span>
    </div>
    
    <div class="pull-right">
      
        
          <span class="post-tag small"><a href="/tags/machine-learning/">#Machine Learning</a></span>
        
      
    </div>
  </div>
  <footer>
    
      
        <div class="delimiter"></div>
        <section class="author-info row">
          <div class="author-avatar col-md-2">
            
          </div>
          <div class="author-meta col-md-10">
            
              <h3 class="author-name text-primary">Inês Gomes</h3>
            
            
              <div class="author-bio">Portuguese Data Scientist exploring this brave new world of Data!</div>
            
            
              <a class="btn btn-primary author-contact" href="mailto:gomes.inesisabel@gmail.com">
                <div>
                  <i class="fa fa-envelope-o" aria-hidden="true"></i>
                  &nbsp;Contact me
                </div>
              </a>
            
          </div>
        </section>
      
	
    
    <div class="delimiter"></div>
    <div class="pager-container">
      
        <a class="btn btn-primary btn-older-posts" href="/posts/ml-end-to-end/">
          <div>
            <span aria-hidden="true">&larr;</span> Older Posts
          </div>
        </a>
      
      
        <a class="btn btn-primary btn-newer-posts disabled" href="#">
          <div>
            Newer Posts <span aria-hidden="true">&rarr;</span>
          </div>
        </a>
      
    </div>
    
  </footer>
</article>
<div class="delimiter"></div>

  </main>
  <footer class="container global-footer">
    <div class="copyright-note pull-left">
      
    </div>
    <div class="sns-links hidden-print">
  
    <a href="mailto:gomes.inesisabel@gmail.com">
      <i class="fa fa-envelope"></i>
    </a>
  
  
  
  
  
  
    <a href="https://github.com/inesgomes" target="_blank">
      <i class="fab fa-github"></i>
    </a>
  
  
  
  
  
  
    <a href="https://linkedin.com/in/inesgomes778" target="_blank">
      <i class="fab fa-linkedin"></i>
    </a>
  
  
  <a href="https://www.kaggle.com/inesgomes" target="_blank">
    <i class="fab fa-kaggle"></i>
  </a>
  
  
</div>

  </footer>

  
  
  <script>
    (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
    (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
    m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
    })(window,document,'script','//www.google-analytics.com/analytics.js','ga');
    ga('create', 'AIzaSyBBSed1vYYekbYLJR78IwyRShIiSpi11ac', 'auto');
    ga('send', 'pageview');
  </script>
  
  

  
  

  
</body>
</html>

