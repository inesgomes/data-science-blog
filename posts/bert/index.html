<!DOCTYPE html>
<html lang="en">
<head prefix="og: http://ogp.me/ns#">
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1" />
  <meta property="og:title" content=" BERT: Bidirectional Encoder Representations from Transformers &middot;  Data Science Wiki" />
  
    <meta name="theme-color" content="#hexcolor" />
  
  <meta property="og:site_name" content="Data Science Wiki" />
  <meta property="og:url" content="/posts/bert/" />
  
  
    <meta property="og:type" content="article" />
    
    <meta property="og:article:published_time" content="2019-11-23T15:38:00Z" />
    
      <meta property="og:article:tag" content="NLP" />
    
  

  <title>
     BERT: Bidirectional Encoder Representations from Transformers &middot;  Data Science Wiki
  </title>

  <link rel="alternative stylesheet" href="/css/bootstrap.min.css" />
  
  <link rel="stylesheet" href="/css/all.css">
  <link rel="stylesheet" href="/css/main.css" />
  <link rel="stylesheet" href="/css/github.css" />
  <link rel="stylesheet" href="/css/color-theme.css" />
  <link rel="stylesheet" href="//fonts.googleapis.com/css?family=Source+Sans+Pro:200,300,400" type="text/css">
  <link rel="shortcut icon" href="/images/favicon.ico" />
  <link rel="apple-touch-icon" href="/images/apple-touch-icon.png" />

  

  
</head>
<body>
  
  <header class="global-header"  style="background-image:url(/images/bg-dark.png)">
  
    <section class="header-text">
      <h1><a href="/">Data Science Wiki</a></h1>
      
        <h4 class="tag-line">
          <i>&#34;All data has its beauty, but not everyone sees it &#34; - Damian Mingle </i>
        </h4>
      
      
        <a class="btn btn-default btn-home" href="/">
          <i class="fa fa-angle-left" aria-hidden="true"></i>
          &nbsp;Home
        </a>
      
      
      <div class="navbar-container">
        
          <a class="btn btn-default navbar-item" href="/pages/about">
            About
          </a>
        
          <a class="btn btn-default navbar-item" href="/tags">
            Blog Posts
          </a>
        
          <a class="btn btn-default navbar-item" href="/pages/glossary">
            Glossary
          </a>
        
          <a class="btn btn-default navbar-item" href="/pages/links">
            Study Repository
          </a>
        
        
        <div class="sns-links hidden-print">
  
    <a href="mailto:gomes.inesisabel@gmail.com">
      <i class="fa fa-envelope"></i>
    </a>
  
  
  
  
  
  
    <a href="https://github.com/inesgomes" target="_blank">
      <i class="fab fa-github"></i>
    </a>
  
  
  
  
  
  
    <a href="https://linkedin.com/in/inesgomes778" target="_blank">
      <i class="fab fa-linkedin"></i>
    </a>
  
  
  <a href="https://www.kaggle.com/inesgomes" target="_blank">
    <i class="fab fa-kaggle"></i>
  </a>
  
  
</div>

      </div>
      
      
      
      
      
      
      
      
      
      
      
    </section>
  </header>
  <main class="container">



<article>
  <header class="article-title">
    <h2 class="text-primary">BERT: Bidirectional Encoder Representations from Transformers</h2>
  </header>
  <div class="delimiter"></div>
  <section>
    

<p>With so much hype around BERT, it was time to study it a little bit deeper. BERT is a pre-trained language model for Natural Language Processing (NLP) developed by Google AI Language researchers. A pre-trained language model is a model that was previously trained in a huge corpus and can capture several components of a language such as long-term dependencies, hierarchical relations, or sentiment. These models are effective for improving many NLP tasks. Examples include Question Answering (Q&amp;A), Natural Language Inference (NLI) or Named Entity Recognition (NER).</p>

<h3 id="background">Background</h3>

<p>BERT, as the name discloses, is a Bidirectional Encoder Representations from Transformers. A <strong>Transformer</strong> is an attention mechanism that learns contextual relations between words, or sub-words, in a text. The Transformer includes an encoder that reads the text input and a decoder that produces a prediction for the task. BERT innovates state of the art by transforming unidirectional self-attention into <strong>bidirectional</strong>. In other words, instead of reading the text input sequentially, that is, every token can only attend to context to its left (or right), it reads the entire sequence at once.</p>

<h3 id="architecture">Architecture</h3>

<p>BERT implementation is divided into two steps: <em>pre-training</em> and <em>fine-tuning</em>. The former, train the model in unlabelled data, while the latter initializes the model with the pre-trained parameters and fine-tunes them using labelled data. Next figure describes a BERT input, where <em>E</em> is an embedding, i.e. a vector representation of the token that includes its context.</p>

<table>
<thead>
<tr>
<th align="center"><img src="../images/bert_input.png" alt="input" /></th>
</tr>
</thead>

<tbody>
<tr>
<td align="center"><em>Figure 1: BERT input example [<a href="##Bibliography">1</a>]</em></td>
</tr>
</tbody>
</table>

<p><br></p>

<p>Regarding the BERT pre-training, the authors start by defining a prediction goal. When using unidirectional self-attention models, it is possible to simply predict the next word in a sequence, but in a bidirectional approach, it would allow each word to &ldquo;see itself&rdquo;. Therefore, BERT uses two methods to pre-train the model: <em>Masked LM</em> and <em>Next Sentence Prediction</em>. Masked LM (MLM) method masks some percentage (in this case 15%) of the input tokens randomly. The final hidden vectors corresponding to the mask tokens are fed into an output SoftMax, to predict the masked words based on the context provided by the non-masked words in the sequence. This method drawback is the creation of a mismatch between pre-training and fine-tuning (since the masked token does not appear during fine-tuning). For further details see the [<a href="##Bibliography">1</a>]. On the other hand, in the <em>Next Sentence Prediction</em> BERT improves the understanding of the relationship between two sentences, predicting if the second sentence in the pair is the next in the original document. To achieve it, during training 50% of the time, the next sentence is the subsequent sentence and the other half is a random sentence from the document. Finally, when training the BERT model, Masked LM and Next Sentence Prediction are trained together, to minimize the combined loss function of the two strategies.</p>

<p>After pre-training BERT, the fine-tuning step occurs when training the model to specific tasks (our project). For each task, we simply plug in the task-specific inputs and outputs into BERT and fine-tune all the parameters end-to-end. In this case, we may add new classification layers.</p>

<h3 id="final-remarks">Final Remarks</h3>

<!--, that learn general language representations by fine-tuning all pre-trained parameters -> fine tuning approach -->

<p>In order to better understand BERT, the authors perform a set of experiments concluding that:</p>

<ul>
<li>Model size matters, even at huge scale</li>
<li>With enough training data we achieve better accuracy</li>
</ul>

<div class='delimiter'></div>

<h2 id="extra-resources">Extra Resources</h2>

<ul>
<li>Bert <a href="https://github.com/google-research/bert">GitHub code</a>;</li>
<li>Data Skeptic Podcast series: <a href="https://dataskeptic.com/blog/episodes/2019/bert">BERT</a>, <a href="https://dataskeptic.com/blog/episodes/2019/bert-is-magic">BERT is magic</a> and <a href="https://dataskeptic.com/blog/episodes/2019/bert-is-shallow">BERT is shallow</a>.</li>
</ul>

<h2 id="similar-topics">Similar Topics</h2>

<ul>
<li><a href="https://openai.com/blog/better-language-models/">OpenAI GPT-2</a></li>
<li><a href="https://arxiv.org/abs/1812.10860">ELMo</a></li>
<li>Word embeddings</li>
<li><strong>NEW!</strong> <a href="https://ai.googleblog.com/2020/03/more-efficient-nlp-model-pre-training.html">ELECTRA</a></li>
</ul>

<h2 id="bibliography">Bibliography</h2>

<p><div class='bibliography'>
    [1] Devlin, Jacob, Ming-Wei Chang, Kenton Lee and Kristina Toutanova. “BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding.” NAACL-HLT (2019). <a href="https://arxiv.org/pdf/1810.04805.pdf">https://arxiv.org/pdf/1810.04805.pdf</a>
    <br><br>
    [2] Horev, Rani. “BERT Explained: State of the Art Language Model for NLP.” Medium. Towards Data Science, November 17, 2018. <a href="https://towardsdatascience.com/bert-explained-state-of-the-art-language-model-for-nlp-f8b21a9b6270">https://towardsdatascience.com/bert-explained-state-of-the-art-language-model-for-nlp-f8b21a9b6270</a>.
</div>
<br></p>

  </section>
  
  <div class="clearfix">
    
      <div class="post-date pull-left">
        <span class="small">
          Posted on
          Nov 23, 2019 at 15:38
        </span>
    </div>
    
    <div class="pull-right">
      
        
          <span class="post-tag small"><a href="/tags/nlp/">#NLP</a></span>
        
      
    </div>
  </div>
  <footer>
    
      
        <div class="delimiter"></div>
        <section class="author-info row">
          <div class="author-avatar col-md-2">
            
          </div>
          <div class="author-meta col-md-10">
            
              <h3 class="author-name text-primary">Inês Gomes</h3>
            
            
              <div class="author-bio">Portuguese Data Scientist exploring this brave new world of Data!</div>
            
            
              <a class="btn btn-primary author-contact" href="mailto:gomes.inesisabel@gmail.com">
                <div>
                  <i class="fa fa-envelope-o" aria-hidden="true"></i>
                  &nbsp;Contact me
                </div>
              </a>
            
          </div>
        </section>
      
	
    
    <div class="delimiter"></div>
    <div class="pager-container">
      
        <a class="btn btn-primary btn-older-posts" href="/pages/about/">
          <div>
            <span aria-hidden="true">&larr;</span> Older Posts
          </div>
        </a>
      
      
        <a class="btn btn-primary btn-newer-posts" href="/posts/ml-end-to-end/">
          <div>
            Newer Posts <span aria-hidden="true">&rarr;</span>
          </div>
        </a>
      
    </div>
    
  </footer>
</article>
<div class="delimiter"></div>

  </main>
  <footer class="container global-footer">
    <div class="copyright-note pull-left">
      
    </div>
    <div class="sns-links hidden-print">
  
    <a href="mailto:gomes.inesisabel@gmail.com">
      <i class="fa fa-envelope"></i>
    </a>
  
  
  
  
  
  
    <a href="https://github.com/inesgomes" target="_blank">
      <i class="fab fa-github"></i>
    </a>
  
  
  
  
  
  
    <a href="https://linkedin.com/in/inesgomes778" target="_blank">
      <i class="fab fa-linkedin"></i>
    </a>
  
  
  <a href="https://www.kaggle.com/inesgomes" target="_blank">
    <i class="fab fa-kaggle"></i>
  </a>
  
  
</div>

  </footer>

  
  
  

  
  

  
</body>
</html>

